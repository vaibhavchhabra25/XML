{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from model.resnet import ResNet18\n",
    "from model.resnext import CifarResNeXt\n",
    "from model.wrn import WideResNet\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading student and teacher models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = ResNet18() # student model\n",
    "resnext = CifarResNeXt(cardinality=8, depth=29, num_classes=10)\n",
    "\n",
    "wrn = WideResNet(depth=28, num_classes=10, widen_factor=10, dropRate=0.3)\n",
    "\n",
    "resnet18.load_state_dict(torch.load('./experiments/resnet18_distill/resnext_teacher/best.pth.tar', map_location='cpu')['state_dict'])\n",
    "\n",
    "\n",
    "state_dict = torch.load('./experiments/base_wrn/best.pth.tar', map_location='cpu')['state_dict']\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "# load params\n",
    "wrn.load_state_dict(new_state_dict)\n",
    "\n",
    "\n",
    "state_dict = torch.load('./experiments/base_resnext29/best.pth.tar', map_location='cpu')['state_dict']\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "# load params\n",
    "resnext.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transformer = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "        ])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data-cifar10', train=True, download=True, transform=train_transformer)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_train_batch, smaple_labels_batch = next(iter(trainloader))\n",
    "fig, ax = plt.subplots(1, 5, figsize=(10, 10))\n",
    "for i in range(5):\n",
    "    ax[i].imshow(sample_train_batch[i].numpy().transpose(1, 2, 0))\n",
    "    ax[i].set_title(smaple_labels_batch[i].item())\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in wrn.named_parameters():\n",
    "#     print(name, param.shape)\n",
    "\n",
    "# import torchvision.models as models\n",
    "# model = models.resnet50(pretrained=True)\n",
    "# # print(model.layer4)\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Registering the forward hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x2a53a3890>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# register forward hook before the average pooling layer\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# adding forward hook to the penultimate relu layer to the student and teacher model\n",
    "resnet18.layer4[1].shortcut.register_forward_hook(get_activation('shortcut'))\n",
    "wrn.relu.register_forward_hook(get_activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the weights we will use for class activation mapping\n",
    "def get_fc_params(model, name):\n",
    "    fc_params = torch.tensor([])\n",
    "    for n, param in model.named_parameters():\n",
    "        if n == name:\n",
    "            fc_params = param\n",
    "            print(n, param.shape)\n",
    "    return fc_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_CAM(forward_hooked_model, input, weights, student = True, rescale=True):\n",
    "    if student:\n",
    "        key = 'shortcut'\n",
    "        layer_size = 512\n",
    "        resize_val = 4\n",
    "    else:\n",
    "        key = 'relu'\n",
    "        layer_size = 640\n",
    "        resize_val = 8\n",
    "    predictions = forward_hooked_model(input) # torch.unsqueeze(train_batch[index], 0)\n",
    "    activation[key] = activation[key][0]\n",
    "    activation[key] = activation[key].permute(1, 2, 0)\n",
    "    activation[key] = activation[key].reshape(-1, layer_size)\n",
    "\n",
    "    activation_map = torch.matmul(activation[key], weights)\n",
    "    activation_map = activation_map.reshape(resize_val, resize_val)\n",
    "    \n",
    "    if rescale:\n",
    "        activation_map = activation_map - torch.min(activation_map)\n",
    "        activation_map = activation_map / (torch.max(activation_map) - torch.min(activation_map))\n",
    "        # activation_map = activation_map * 255\n",
    "\n",
    "    return activation_map, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc.weight torch.Size([10, 640])\n"
     ]
    }
   ],
   "source": [
    "fc_params_teacher = get_fc_params(wrn, 'fc.weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 3, 32, 32]) torch.Size([1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:36<00:00, 10.38it/s]\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(trainloader)\n",
    "_batch = next(train_iter)\n",
    "current_input_batch = _batch[0]\n",
    "current_label_batch = _batch[1]\n",
    "print(current_input_batch.shape, current_label_batch.shape)\n",
    "\n",
    "train_transformer_2 = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "complexity_values_teacher = []\n",
    "drop_values_teacher = []\n",
    "ssim_score_teacher = []\n",
    "adcc_score_teacher = []\n",
    "\n",
    "for i in tqdm(range(current_input_batch.shape[0])):\n",
    "    activation_map, predictions = get_weighted_CAM(wrn, torch.unsqueeze(current_input_batch[i], 0), fc_params_teacher[current_label_batch[i]], student=False)\n",
    "    activation_map = activation_map.detach().numpy() # CAMc(x)\n",
    "\n",
    "    complexity_values_teacher.append(np.linalg.norm(activation_map, ord=1))\n",
    "\n",
    "    activation_map = cv2.resize(activation_map, (32, 32)) # 32 x 32\n",
    "\n",
    "    original_image = current_input_batch[i].numpy()\n",
    "    original_image = original_image - np.min(original_image)\n",
    "    original_image = original_image / np.max(original_image)\n",
    "    original_image = original_image * 255\n",
    "    original_image = original_image.astype(np.uint8)\n",
    "\n",
    "    occluded_image = original_image * np.stack((activation_map, activation_map, activation_map)) # 3 X 32 x 32\n",
    "    occluded_image -= np.min(occluded_image)\n",
    "    occluded_image /= np.max(occluded_image)\n",
    "    occluded_image = occluded_image.astype(float) # 3 x 32 x 32\n",
    "\n",
    "    occluded_image = torch.tensor(occluded_image).float()\n",
    "    occluded_image = train_transformer_2(occluded_image)\n",
    "    \n",
    "    occluded_activations, occluded_predictions = get_weighted_CAM(wrn, torch.unsqueeze(occluded_image, 0), fc_params_teacher[current_label_batch[i]], student=False)\n",
    "    occluded_activations = occluded_activations.detach().numpy()\n",
    "    occluded_activations = cv2.resize(occluded_activations, (32, 32)) # 32 x 32\n",
    "    # occluded_predictions = wrn(torch.unsqueeze(occluded_image, 0))\n",
    "\n",
    "    prediction_label_from_original_image = torch.argmax(predictions, dim=1).item()\n",
    "    prediction_label_from_occluded_image = torch.argmax(occluded_predictions, dim=1).item()\n",
    "\n",
    "    prob_pred_from_original_image = torch.max(predictions, dim=1).values.item()\n",
    "    prob_pred_from_occluded_image = torch.max(occluded_predictions, dim=1).values.item()\n",
    "    fraction_drop = (prob_pred_from_original_image - prob_pred_from_occluded_image) / prob_pred_from_original_image\n",
    "\n",
    "    drop_values_teacher.append(max(0, fraction_drop))\n",
    "    ssim_score_teacher.append(ssim(activation_map, occluded_activations, data_range=occluded_activations.max() - occluded_activations.min()))\n",
    "\n",
    "    adcc_score = 3 / (1/ssim_score_teacher[-1] + 1/(1 - complexity_values_teacher[-1]) + 1/(1 - drop_values_teacher[-1]))\n",
    "    adcc_score_teacher.append(adcc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For teacher model, Mean complexity values:  5.655658\n",
      "For teacher model, Mean drop values:  0.12278772875384615\n",
      "For teacher model, Mean SSIM values:  0.5767710451610357\n",
      "For teacher model, Mean Final ADCC values:  1.0776004227220892\n"
     ]
    }
   ],
   "source": [
    "print(\"For teacher model, Mean complexity values: \", np.mean(complexity_values_teacher))\n",
    "print(\"For teacher model, Mean drop values: \", np.mean(drop_values_teacher))\n",
    "print(\"For teacher model, Mean SSIM values: \", np.mean(ssim_score_teacher))\n",
    "print(\"For teacher model, Mean Final ADCC values: \", np.mean(adcc_score_teacher))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weight torch.Size([10, 512])\n"
     ]
    }
   ],
   "source": [
    "fc_params_student = get_fc_params(resnet18, 'linear.weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:24<00:00, 40.21it/s]\n"
     ]
    }
   ],
   "source": [
    "complexity_values_student = []\n",
    "drop_values_student = []\n",
    "ssim_score_students = []\n",
    "adcc_score_student = []\n",
    "\n",
    "for i in tqdm(range(current_input_batch.shape[0])):\n",
    "    activation_map, predictions = get_weighted_CAM(resnet18, torch.unsqueeze(current_input_batch[i], 0), fc_params_student[current_label_batch[i]], student=True)\n",
    "    activation_map = activation_map.detach().numpy() # CAMc(x)\n",
    "\n",
    "    complexity_values_student.append(np.linalg.norm(activation_map, ord=1))\n",
    "\n",
    "    activation_map = cv2.resize(activation_map, (32, 32)) # 32 x 32\n",
    "\n",
    "    original_image = current_input_batch[i].numpy()\n",
    "    original_image = original_image - np.min(original_image)\n",
    "    original_image = original_image / np.max(original_image)\n",
    "    original_image = original_image * 255\n",
    "    original_image = original_image.astype(np.uint8)\n",
    "\n",
    "    occluded_image = original_image * np.stack((activation_map, activation_map, activation_map)) # 3 X 32 x 32\n",
    "    occluded_image -= np.min(occluded_image)\n",
    "    occluded_image /= np.max(occluded_image)\n",
    "    occluded_image = occluded_image.astype(float) # 3 x 32 x 32\n",
    "\n",
    "    occluded_image = torch.tensor(occluded_image).float()\n",
    "    occluded_image = train_transformer_2(occluded_image)\n",
    "\n",
    "    occluded_activations, occluded_predictions = get_weighted_CAM(resnet18, torch.unsqueeze(occluded_image, 0), fc_params_student[current_label_batch[i]], student=True)\n",
    "    occluded_activations = occluded_activations.detach().numpy()\n",
    "    occluded_activations = cv2.resize(occluded_activations, (32, 32)) # 32 x 32\n",
    "    # occluded_predictions = resnet18(torch.unsqueeze(occluded_image, 0))\n",
    "\n",
    "    prediction_label_from_original_image = torch.argmax(predictions, dim=1).item()\n",
    "    prediction_label_from_occluded_image = torch.argmax(occluded_predictions, dim=1).item()\n",
    "\n",
    "    prob_pred_from_original_image = torch.max(predictions, dim=1).values.item()\n",
    "    prob_pred_from_occluded_image = torch.max(occluded_predictions, dim=1).values.item()\n",
    "    fraction_drop = (prob_pred_from_original_image - prob_pred_from_occluded_image) / prob_pred_from_original_image\n",
    "\n",
    "    drop_values_student.append(max(0, fraction_drop))\n",
    "    ssim_score_students.append(ssim(activation_map, occluded_activations, data_range=occluded_activations.max() - occluded_activations.min()))\n",
    "\n",
    "    adcc_score = 3 / (1/ssim_score_students[-1] + 1/(1 - complexity_values_student[-1]) + 1/(1 - drop_values_student[-1]))\n",
    "    adcc_score_student.append(adcc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For student model, Mean complexity values:  2.7335796\n",
      "For student model, Mean drop values:  0.08477124724789933\n",
      "For student model, Mean SSIM values:  0.6687124108280107\n",
      "For student model, Mean Final ADCC values:  1.482402559243952\n"
     ]
    }
   ],
   "source": [
    "print(\"For student model, Mean complexity values: \", np.mean(complexity_values_student))\n",
    "print(\"For student model, Mean drop values: \", np.mean(drop_values_student))\n",
    "print(\"For student model, Mean SSIM values: \", np.mean(ssim_score_students))\n",
    "print(\"For student model, Mean Final ADCC values: \", np.mean(adcc_score_student))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "\n",
    "# for i in range(5):\n",
    "#     activation_map, _ = get_weighted_CAM(wrn, torch.unsqueeze(sample_train_batch[i], 0), fc_params_teacher[smaple_labels_batch[i]], student=False)\n",
    "#     print(activation_map.shape)\n",
    "#     activation_map = activation_map.detach().numpy().astype(np.uint8)\n",
    "#     activation_map *= 255\n",
    "    \n",
    "#     activation_map = cv2.resize(activation_map, (224, 224))\n",
    "#     activation_map = cv2.applyColorMap(activation_map, cv2.COLORMAP_JET)\n",
    "#     activation_map = cv2.cvtColor(activation_map, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "#     original_image = sample_train_batch[i].numpy().transpose(1, 2, 0)\n",
    "#     original_image = original_image - np.min(original_image)\n",
    "#     original_image = original_image / np.max(original_image)\n",
    "#     original_image = original_image * 255\n",
    "#     original_image = original_image.astype(np.uint8)\n",
    "#     original_image = cv2.resize(original_image, (224, 224))\n",
    "\n",
    "#     activation_map = cv2.addWeighted(original_image, 0.5, activation_map, 0.5, 0)\n",
    "\n",
    "#     axes[0][i].imshow(original_image)\n",
    "#     axes[0][i].axis('off')\n",
    "#     axes[1][i].imshow(activation_map)\n",
    "#     axes[1][i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "\n",
    "# for i in range(5):  \n",
    "#     activation_map, _ = get_weighted_CAM(resnet18, torch.unsqueeze(sample_train_batch[i], 0), fc_params_student[smaple_labels_batch[i]], student=True)\n",
    "\n",
    "#     activation_map = activation_map.detach().numpy().astype(np.uint8)\n",
    "\n",
    "#     activation_map = cv2.resize(activation_map, (224, 224)) # 224 x 224\n",
    "#     activation_map = cv2.applyColorMap(activation_map, cv2.COLORMAP_JET)\n",
    "#     activation_map = cv2.cvtColor(activation_map, cv2.COLOR_BGR2RGB)\n",
    "#     original_image = sample_train_batch[i].numpy().transpose(1, 2, 0)\n",
    "#     original_image = original_image - np.min(original_image)\n",
    "#     original_image = original_image / np.max(original_image)\n",
    "#     original_image = original_image * 255\n",
    "#     original_image = original_image.astype(np.uint8)\n",
    "\n",
    "#     original_image = cv2.resize(original_image, (224, 224))\n",
    "\n",
    "#     activation_map = cv2.addWeighted(original_image, 0.5, activation_map, 0.5, 0)\n",
    "\n",
    "#     axes[0][i].imshow(original_image)\n",
    "#     axes[0][i].axis('off')\n",
    "#     axes[1][i].imshow(activation_map)\n",
    "#     axes[1][i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "# activation_maps_teacher = []\n",
    "# threshold = 0.6\n",
    "# occluded_images = []\n",
    "\n",
    "# for i in range(5):\n",
    "#     activation_map, _ = get_weighted_CAM(wrn, torch.unsqueeze(sample_train_batch[i], 0), fc_params_teacher[smaple_labels_batch[i]], student=False)\n",
    "#     activation_map = activation_map.detach().numpy().astype(np.uint8)\n",
    "#     activation_map = cv2.resize(activation_map, (32, 32))\n",
    "\n",
    "#     original_image = sample_train_batch[i].numpy().transpose(1, 2, 0)\n",
    "#     original_image = original_image - np.min(original_image)\n",
    "#     original_image = original_image / np.max(original_image)\n",
    "\n",
    "#     occluded_image = original_image * np.stack((activation_map, activation_map, activation_map)).transpose(1, 2, 0)\n",
    "\n",
    "#     original_image = original_image * 255\n",
    "#     original_image = original_image.astype(np.uint8)\n",
    "\n",
    "#     # activation_map = np.ones((32, 32))\n",
    "#     # occluded_image = original_image * np.stack((activation_map, activation_map, activation_map)).transpose(1, 2, 0)\n",
    "#     occluded_image -= np.min(occluded_image)\n",
    "#     occluded_image /= np.max(occluded_image)\n",
    "#     occluded_image *= 255\n",
    "#     occluded_image = occluded_image.astype(np.uint8)\n",
    "\n",
    "\n",
    "#     axes[0][i].imshow(original_image)\n",
    "#     axes[0][i].axis('off')\n",
    "#     axes[1][i].imshow(occluded_image)\n",
    "#     axes[1][i].axis('off')\n",
    "\n",
    "#     occluded_image = occluded_image.astype(float)\n",
    "#     occluded_image -= np.min(occluded_image)\n",
    "#     occluded_image /= np.max(occluded_image)\n",
    "#     occluded_images.append(occluded_image.transpose(2, 0, 1))\n",
    "\n",
    "# occluded_images = torch.tensor(occluded_images).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(resnet18, (3, 32, 32))\n",
    "# summary(wrn, (3, 32, 32))\n",
    "# summary(resnext, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(occluded_images.shape, occluded_image.dtype)\n",
    "\n",
    "# # samp = occluded_images[0]\n",
    "# # plt.imshow(samp.numpy().transpose(1, 2, 0).astype(np.uint8))\n",
    "\n",
    "# # print(occluded_images[0].shape)\n",
    "# # plt.imshow(occluded_images[0].numpy().transpose(1, 2, 0).astype(np.uint8))\n",
    "# train_transformer_2 = transforms.Compose([\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "# ])\n",
    "# occluded_images = train_transformer_2(occluded_images)\n",
    "# # temp = train_transformer_2(occluded_images[0])\n",
    "# # print(temp.shape)\n",
    "\n",
    "# wrn.eval()\n",
    "# true_prediction_probability = wrn(sample_train_batch[:5])\n",
    "# occluded_prediction_probability = wrn(occluded_images)\n",
    "# # occluded_prediction_probability = wrn(temp.unsqueeze(0))\n",
    "# print(torch.argmax(occluded_prediction_probability, dim=1), smaple_labels_batch[:5], torch.argmax(true_prediction_probability, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI3000_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
